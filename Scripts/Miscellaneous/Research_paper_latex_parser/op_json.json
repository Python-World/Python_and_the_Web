[
    {
        "author": "I.M. Great and So.R. Yu",
        "title": "A Sample Research Paper",
        "Introduction": " \n \n Using latex is pretty easy if you have a sample document you can follow.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed volutpat ornare odio et faucibus. Donec fringilla massa eget auctor viverra. Mauris a imperdiet est. Cras tincidunt nulla ut elit tristique ultricies. Phasellus nec orci vel mi suscipit maximus at vitae tortor. Vivamus sed libero vel lacus aliquam rhoncus. Ut in lacinia nunc. Nullam quis mauris leo. Phasellus vitae nisl condimentum quam congue volutpat. Quisque et dapibus ipsum. Curabitur fringilla pellentesque elit, non posuere purus malesuada id. Pellentesque rutrum vitae urna eu mattis.\n \n Maecenas ac congue massa. Quisque a sem turpis. Duis et diam ex. Suspendisse et enim interdum, sodales risus eu, ultrices est. Suspendisse eu odio enim. In vulputate odio porttitor tincidunt vestibulum. Praesent tincidunt ullamcorper purus, quis semper felis volutpat quis.\n \n ",
        "Results": " \n Including figures, tables, and equations is easy. Latex also permits easy reference to document elements (figures, tables, sections). Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tincidunt lorem luctus eros dictum faucibus. Fusce euismod libero et erat pretium dapibus. Pellentesque faucibus hendrerit est, ac fringilla urna. In porta, ante eu dictum vestibulum, nisl nulla euismod purus, ac bibendum nibh ante vel elit. Fusce diam ante, tincidunt id eleifend a, hendrerit vitae tellus. Duis pretium urna ac vestibulum eleifend. Suspendisse potenti. Aliquam varius odio in pretium semper. Ut faucibus lobortis mauris vel sollicitudin. Nullam condimentum, lacus quis mattis pellentesque, massa nulla cursus nisi, aliquet eleifend est tellus ut libero.\n \n  \n \n  \n \n  \n \n ",
        "Conclusions": " \n \n Man, latex is great! Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tincidunt lorem luctus eros dictum faucibus. Fusce euismod libero et erat pretium dapibus. Pellentesque faucibus hendrerit est, ac fringilla urna. In porta, ante eu dictum vestibulum, nisl nulla euismod purus, ac bibendum nibh ante vel elit. Fusce diam ante, tincidunt id eleifend a, hendrerit vitae tellus. Duis pretium urna ac vestibulum eleifend. Suspendisse potenti. Aliquam varius odio in pretium semper. Ut faucibus lobortis mauris vel sollicitudin. Nullam condimentum, lacus quis mattis pellentesque, massa nulla cursus nisi, aliquet eleifend est tellus ut libero.\n \n ",
        "Some_title": " \n \n Test title for user defined  section.\n \n ",
        "user_defined_title_for_begin": " \n \n wjlrhfwer ljqr flwuer j rlferfurl u airlf  aiurf uoiruf iuoqir oiuqr iuq woe\n ",
        "acknowledgement": "\n The author is grateful to Donald Knuth for inventing tex, and making publication quality typesetting a reality for scientists around the world.\n \n "
    },
    {
        "author": "Pratik Merchant, Smit Moradiya, Jignesh Nagda, Niket Mehta",
        "title": "Parallel Implementation of Support Vector Machine",
        "Introduction": " \n \n The name \u00e2\u0080\u0098support vectors\u00e2\u0080\u0099 (data points) used to define this dividing plane. Since we only require the SVs to create a classifier, the non SVs can be discarded. However, it becomes a problem when the points are not separable by a simple linear plane. Hence, to handle this problem, SVM uses what is known as the \u00e2\u0080\u009ckernel trick\u00e2\u0080\u009d on the training data and the mapping to a higher dimensional space is done by it, where such a dividing plane can be found more easily. Every improve accuracy. The role of Kernel is to transform the problem using some linear algebra for linear SVM and this is how the learning of the hyperplane happens. To avoid misclassifying each training example is given by the regularisation parameter. Lower is the regularisation value, more is the misclassification. To decide how far the influence of each training parameter reaches, the gamma parameter is used. Low gamma value means points which are at a far distance from the separation line are considered for calculation whereas a high gamma value implies that only the points nearby to the separation line are considered for calculation. Lastly, the separation of the line/hyperplane to the point which closest to it is called as margin. A larger separation for both the classes means a good margin and also no crossing into other class.\n The steps to implement SVM are as follows: Step 1: Import all the necessary libraries such as numpy, pandas, matplotlib. Step 2: Importing the dataset Step 3: Performing exploratory data analysis Step 4: Performing data pre-processing Step 5: Splitting the data into train and test. Step 6: Import SVM, create a classifier and train the model. Step 6: Making predictions Step 7: Evaluating the algorithm  Step 8: Results \n \n ",
        "my_section": " \n \n nlwekndw lweuidwe nulei nameiude includewe oiu wede oiuwe dn eiuqwend\n \n ",
        "Results": " \n PSVM first loads the training data onto \u00e2\u0080\u0098m\u00e2\u0080\u0099 number of machines. This done in round robin fashion. The memory required by each memory is Big-Oh of nd/m. In the next step, PSVM performs a row-based ICF parallely on the data which has been loaded. At the end of this step, only a small portion of the factorized matrix is stored on each machine, which has a space complexity of Big-Oh of np/m. For the quadratic optimization problem, IPM is performed hereafter. Let, n- no. of training instances d-initial no. of dimensions p- After factorization, reduced matrix dimension (p<<<n) m- number of machines With the help of PSVM, the memory requirements reduce from a complexity of Big-Oh of n^2 to Big-Oh of np/m.  \n  \n HDF5 which is a file format, data model, and a set of software allows users to store data and associated metadata. \n \n Several of its features make it useful for high performance computing and big data applications, such as data compression, which along with binary value storage can greatly reduce file sizes. It also allows parallel file access, which enables one or more processors to read/write data from a single file, and features customizable data \u00e2\u0080\u009cchunking\u00e2\u0080\u009d, which allows users to define how the data is internally arranged into subsets. This can be used to tune parallel performance. \n \n An open source cluster computing system intended for the analysis and processing of big data is Apache Spark. Spark clusters have the ability to scale up to thousands of distributed nodes that can work cooperatively to process very large datasets in parallel, this makes them an example of an HTC system. It also supports real-time streaming processing of data as well as real-time streaming processing of data. Some of its main components are Spark streaming, Spark SQL, Spark core which are used for stream processing, to enable structured data processing and handle task distribution and scheduling respectively. (mllib) Machine Learning Library supports many tasks such as linear SVM training,linear regression and clustering. However some extra code needs to be written to handle multi-class problems since it only supports binary classification. \n  \n There maybe a period of CPU idle time if one segment finishes training before it concatenation counterpart. The issue can be cumulative in severely distributed databases. The benefits of attempting to balance the segments after the processing has started are likely to be outweighed by the overhead in MPI communication. This difference should be mitigated by proper randomization of the input vector order. \n  \n Applications SVMs have been found particularly useful in earth observation and satellite imagery data analysis. Some of its other applications include: 1. Detetction of faces 2. Categorization of text and as well as hypertext 3. Image classifier 4. Fields related to biology 5. Remote homo-logy detection 6. Recognition of hand-written characters 7. GPC 8. Geo and Environmental Sciences \n \n ",
        "Conclusions": " \n \n Many algorithmic approaches are found to be more effective when kernels are not used or memory is not a constraint. Also, other approaches can be used to achieve good speedup by dividing a serial algorithm into subtasks. These subtasks are basically subsets of the training data. If no. of machines continue to expand and cross the data-size independent threshold, PSVM cannot achieve linear speedup in such cases. There are 2 types of overheads encountered while implementing parallel SVM- communication and synchronization overheads. During message passing, communication time is accounted for.  Computation, communication and synchronization together form the running time. To increase the accuracy, PSVM must select the correct no. of machines.  Hence, we can conclude by saying that when the parallelism of modern hardware is leveraged, massive speedups are possible a satisfactory performance is achieved\n \n ",
        "acknowledgement": "\n I am grateful to my college professors for providing me this wonderful oppurtunity to present this research paper.\n \n "
    },
    {
        "author": "Pratik Merchant",
        "title": "Prediction of human behaviour with the aid of sentiment analysis using social media datasets.",
        "Introduction": " \n \n Social media data like Facebook, Twitter, Instagram blogs, etc. is currently growing in an exploding speed. Sentiment analysis\u00e2\u0080\u0093also called opinion mining\u00e2\u0080\u0093is the process of defining and categorizing opinions in a given piece of text as positive, negative, or neutral. The main purpose of conducting this research is to understand the sentiments which in turn can help us mine knowledge and capture the ideas without necessarily going through all data, which will save us a huge amount of time. Also, this analysis can further be used for a variety of purposes such as identifying influencers, competitive benchmarking, consumer opinion and brand sentiment, etc.\n The already existing models lack accuracy. Also, they predict on the basis of one or 2 factors which is too less a number considering the amount of thought processes a human brain goes through before coming on to a decision. Also, the inaccuracy occurring due to the automated bots need to be taken into consideration. Since, they can largely tilt the dataset to a particular side (positive or negative). \n The main goal is to improve accuracy and also to remove the input of the bots from the datasets using appropriate filtering techniques. And also, to merge the prediction of all the various datasets together to obtain a cumulative prediction of all the social media accounts a person uses.\n The Government or the common public can largely benefit from this since any negative event(protests) if predicted by the model may help in taking adequate protective measures and hence in turn maybe avoid or reduce the magnitude of the same. This issue if addressed before could have prevented the negative impacts of a lot of events such as the Muzaffarnagar riots, FTII Agitation, Pro-Jallikattu protests which took place in Tamil Nadu, etc. Hence, any such events if again predicted in the future, can very well be avoided by taking appropriate advance action.   \n \n ",
        "Results": " \n There are three machine learning classification algorithms that are predominantly used for sentiment analysis in social media and they are as follows:\n a.\tSupport Vector Machines (SVMs)\n b.\tNaive-bayes\n c.\tDecision Trees\n Each has it\u00e2\u0080\u0099s own advantages and drawbacks; however, a few different studies have concluded that the Naive-Bayes classifier is the more accurate of the three.[1]\n       \n Naive-Bayes classifier is a machine learning classification algorithm that asserts an independent value for each feature within a dataset. In other words, each element is valued individually to determine a probability that the sum of these values will constitute a pre-defined label or outcome. Effective sentiment analysis of social media datasets using Naive Bayesian Classification involves extraction of subjective information from textual data. A normal human can easily understand the sentiment of a document written in natural language based on its knowledge of understanding the polarity of words and in some cases the general semantics used to describe the subject. The project aims to make the machine extract the polarity (positive, negative or neutral) of social media dataset with respect to the queried keyword.\n This project introduces an approach for automatically classifying the sentiment of social media data by using the following procedure: First the training data is fed to the sentiment analysis engine for learning by using machine learning algorithm. The next step is to filter misleading data(mostly encountered because of bots).The next step involved is the training of the dataset by mathematical formulations. After the learning is complete with qualified accuracy, the machine starts accepting individual social data with respect to keyword that it analyses and interprets, and then classifies it as positive, negative or neutral with respect to the query term.[2] The prediction of an individual once obtained from the different social media datasets may then be cumulated and then compared with the prediction of other individuals to see if there is anything in common. Common predictions if found any may indicate the mass sentiment of the people and will also hint about their future course of actions if any.\n When talking about textual sentiment analysis, this usually comes in the form of a training set bag-of-words already sorted into positive or negative categories. A positive word may have a +1 scoring while a negative word will have a -1 scoring. You can also assign higher values to certain words that may be more negative in degree. Regardless, if the final score of a mention is positive, then the mention is positive and vice versa for negative final score.\n If word only appears once, we don\u00e2\u0080\u0099t need a frequency table. If we assign each positive and negative value a \u00e2\u0080\u009c1\u00e2\u0080\u009d, then we can simply divide the positive and negative words by the amount of words in the entire mention and then the subtract the negative words score from the positive one and  if the total of our mention comes out as positive, we can say the sentiment of the mention above is positive and vice versa for a negative result.\n Since the total of our mention comes out as positive, we can say the sentiment of the mention above is positive. This is a pretty clear-cut case as we didn\u00e2\u0080\u0099t encounter polarizing words that might skew the result if a computer can\u00e2\u0080\u0099t understand which category the word belongs to.[1] \n \n Now, the maxim that more data will lead to better predictive models is not always true, because noise in the data can overwhelm predictive models. The ability to deal with noisy, incomplete, and inconsistent data will be at the heart of next-generation predictive models. For instance, when identifying \u00e2\u0080\u009cbots\u00e2\u0080\u009d on Twitter that are seeking to sway opinion to be positive about a political candidate, we needed to ignore the huge numbers of bots that were seeking to achieve other ends- such as spreading spam or seeking to influence opinions about other topics or to deceive users into clicking on links that generate revenue for the person who included that link in their tweet. Moreover, data about many Twitter handles are limited and, in some cases, intentionally misleading. Bot developers go to considerable effort to ensure that their bots elude detection.\n \n The generation and reduction to practice of robust multistage predictive modeling for emergent phenomena is an important step. For instance, social movements have been classified into five stages: genesis of the movement, increase in social unrest, enthusiastic mobilization to develop an organization, maintenance of the organization, and termination (when the movement starts to die down). When the protest is in an early stage (for example, of people expressing grievances on Twitter), some stakeholders would benefit from a prediction of the likelihood of violence occurring in any of the future stages. In such extreme cases, identifying bots is a very important part.\n In this way, the above proposed methodology if implemented, can be of great help in a variety of applications as seen above.\n \n \n ",
        "Conclusions": " \n \n Ultimately, once can say that sentiment analysis isn\u00e2\u0080\u0099t perfect, but neither are we when trying to decipher what someone means. Within social media monitoring, we need sentiment analysis as a starting point to understand general public sentiment in aggregate. \n Hence, we can say that social media is perhaps the largest pool from which we can mine for public opinion and begin to gather informative data for prediction purposes. \n In this way, I plan to complete the above mentioned process as soon as possible once begun. If done correctly, the process would be completed within a stipulated period of time. If I am successful in meeting my objectives then, this shall be largely benefical to the Government authorities, the Police authorities as well as the common people at large. \n \n ",
        "acknowledgement": "\n I would like to thank my college professors for supporting me immensely in this endeavor.\n \n "
    },
    {
        "author": "\n Ameya Keskar                                                         Priya Mane\n Chinmay Lotankar                                                     Jeet Mehta\n ",
        "title": "C4.5 CLASSIFICATION ALGORITHM",
        "Introduction": " \n \n Data mining is the process of analyzing large data and getting valuable information from it. There are various algorithms and techniques done to do so. One such data structure is Decision tree; it is a flowchart-like structure with nodes and arrows directing from one node to another. At each node, one attribute is considered and further split branches equal to the number of unique values the attribute can take. Each of this branch is connected to other node where the value of next attribute is defined. Hence in going from one node to another, we fix or determine the value of each attribute. Each leaf node consists of one of the values of classification variable.\n Now the question is which attribute must be placed at what level in a tree.C4.5 Algorithm is used to choose the attributes to be placed at each level of the tree. The main advantage of C4.5 algorithm can deal with attributes having numeric data/non-categorical data which is difficult to classify per say and also deal with missing value data.C4.5 algorithm makes a decision tree by using the concept of information entropy. The parameter used here is normalized information gain. Normalized information gain is calculated for each attribute and the one with maximum value is chosen as the first attribute/root node of the decision tree.\n \n \n literature review\n \n For the construction of a decision tree, we can use the C4.5 algorithm. The algorithm is based on Information gain entropy. We can say that, if an event is highly probable, there is no surprise if it occurs. This means that it gives very little information. This means amount of information gained is inversely proportional to the probability of the event. Entropy is proportional to the probability of an event; hence we can also say that information gain and entropy are inversely proportional.\n In decision trees, it is necessary that with each split the entropy decreases. Hence, if the splitting is done accurately, we may arrive to a very definite decision. So, we check each node for all possible splitting. cases for First, we calculate the entropy difference and the case for which difference is least is considered..\n \n ALGORITHM:\n Calculate Information gain for each parameter.\n Determine the attribute with maximum Information gain entropy.\n Choose this attribute as next splitting node.\n Continue in similar manner for all attributes.\n  \n Let us consider an example \n \n  \n \n ",
        "Results": " \n C4.5 Vs C5.0 C4.5 was superseded in 1997 by a commercial system See5/C5.0 (C5.0 for Unix / Linux, See5 pour Windows).  \n The changes hold within new capabilities as well as much improved efficiency, and include: \n   A variant of boosting that constructs an ensemble of classifiers which are then voted to give a final classification. This often leads to a dramatic improvement in predictive accuracy.   New data types (e.g., dates), \u00e2\u0080\u009cnot applicable\u00e2\u0080\u009d values, variable misclassification costs, and mechanisms to pre-filter attributes. \n  Unordered rule set when a case is classified, all applicable rules are found and voted. \n  This improves both their predictive accuracy and the interpretability of rule sets.  \n Multi-threading enhances scalability. C5.0 have the ability to take advantage of computers with multiple CPUs and/or cores\n \n \n ",
        "Conclusions": " \n \n The decision tree is a usual algorithm in data mining.C4.5 algorithm is a wide application scope, high frequency decision tree algorithm. It constructs and prunes the decision tree analysis and estimates, completes the classified data mining by data preprocessing and choosing parameters or catalog.The article analyzes the C4.5 and improved methods for the calculation speed of C4.5 algorithm in detail. At least, it is proved by experiment data set that the improved C4.5 algorithm is well-performed on the training speed classify and accuracy. In this Paper C4.5 algorithm was improved the experiment proved that it has minimal impact on the classification accuracy, but the efficiency increased a lot. We can not only speed up the growing of the decision tree, so that better information of rules can be generated. In this paper the algorithm was verified by different large datasets which are publicly available on UCI\n machine learning repository. With the improved algorithm ,we can get faster and more effective results without the change of the final decision and the presented algorithm constructs the decision tree more clear and understandable .Efficiency and classification is greatly improved and the disadvantages of low efficiency and memory consumption while dealing with large amount of data were overcome as it was in C4.5.If the amount of data is small original C4.5 is\n used because of its higher accuracy.\n \n \n ",
        "thebibliography": " \n \n https://towardsdatascience.com/what-is-the-c4-5-algorithm-and-how-does-it-work-2b971a9e7db0\n https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb\n https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/\n https://arxiv.org/abs/1310.2071\n https://www.sciencedirect.com/science/article/pii/S0925231298000903\n https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/\n \n \n \n ",
        "acknowledgement": "\n The authors are grateful to K.J Somaiya college of Engineering faculty.\n \n "
    },
    {
        "author": "\n Arman Cohan\n Franck Dernoncourt\n Doo Soon Kim\n Trung Bui\n Seokhwan Kim \n Walter Chang\n Nazli Goharian\u00e2\u0080\u00a0\n ",
        "title": "A Discourse-Aware Attention Model for\n Abstractive Summarization of Long Documents",
        "Introduction": " \n Existing large-scale summarization datasets\n consist of relatively short documents. For exam\u0002ple, articles in the CNN/Daily Mail dataset (Her\u0002mann et al., 2015) are on average about 600 words\n long. Similarly, existing neural summarization\n models have focused on summarizing sentences\n and short documents. In this work, we propose a\n model for effective abstractive summarization of\n longer documents. Scientific papers are an ex\u0002ample of documents that are significantly longer\n than news articles (see Table 1). They also fol\u0002low a standard discourse structure describing the\n problem, methodology, experiments/results, and\n finally conclusions (Suppe, 1998).\n Most summarization works in the literature\n focus on extractive summarization. Examples\n of prominent approaches include frequency-based\n methods (Vanderwende et al., 2007), graph-based\n methods (Erkan and Radev, 2004), topic mod\u0002eling (Steinberger and Jezek, 2004), and neural\n models (Nallapati et al., 2017). Abstractive sum\u0002marization is an alternative approach where the\n generated summary may contain novel words and\n phrases and is more similar to how humans sum\u0002marize documents (Jing, 2002). Recently, neu\u0002ral methods have led to encouraging results in\n abstractive summarization (Nallapati et al., 2016;\n See et al., 2017; Paulus et al., 2017; Li et al.,\n 2017). These approaches employ a general frame\u0002work of sequence-to-sequence (seq2seq) models\n (Sutskever et al., 2014) where the document is\n fed to an encoder network and another (recurrent)\n network learns to decode the summary. While\n promising, these methods focus on summarizing\n news articles which are relatively short. Many\n other document types, however, are longer and\n structured. Seq2seq models tend to struggle with\n longer sequences because at each decoding step,\n the decoder needs to learn to construct a context\n vector capturing relevant information from all the\n tokens in the source sequence (Shao et al., 2017).\n Our main contribution is an abstractive model\n for summarizing scientific papers which are an\n example of long-form structured document types.\n Our model includes a hierarchical encoder, captur\u0002ing the discourse structure of the document and a\n discourse-aware decoder that generates the sum\u0002mary. Our decoder attends to different discourse\n sections and allows the model to more accurately\n represent important information from the source\n resulting in a better context vector. We also in\u0002troduce two large-scale datasets of long and struc\u0002tured scientific papers obtained from arXiv and\n PubMed to support both training and evaluating\n models on the task of long document summariza\u0002tion. Evaluation results show that our method out\u0002performs state-of-the-art summarization models1.\n \n \n ",
        "Results": " \n Our main results are shown in Tables 2\n and 3. Our model significantly outperforms the\n state-of-the-art abstractive methods, showing its\n effectiveness on both datasets. We observe that\n in our ROUGE-1 score is respectively about 4 and\n 3 points higher than the abstractive model PntrGen-Seq2Seq for the arXiv and PubMed datasets,\n providing a significant improvement. Our method\n also outperforms most of the extractive methods\n except for LexRank in one of the ROUGE scores.\n We note that since extractive methods copy salient\n sentences from the document, it is usually easier for them to achieve higher ROUGE scores.\n Figure 2 illustrates the effectiveness of our\n model extensions in capturing various discourse\n information from the papers. It can be observed\n that the state-of-the-art Pntr-Gen-Seq2Seq model\n generates a summary that mostly focuses on introducing the problem, whereas our model generates\n a summary that includes more information about\n the methodology and impacts of the target paper.\n This indicates that the context vector in our model\n compared with Pntr-Gen-Seq2Seq is better able to\n capture important information from the source by\n attending to various discourse sections.\n \n \n ",
        "Conclusions": " \n This work was the first attempt at addressing\n neural abstractive summarization of single, long\n documents. We presented a neural sequence-tosequence model that is able to effectively summarize long and structured documents such as scientific papers. While our results are encouraging,\n there is still much room for improvement for this\n challenging task; our new datasets can help the\n community to further explore this problem.\n We note that following the convention in the\n summarization research, our quantitative evaluation is performed by ROUGE automatic metric.\n While ROUGE is an effective evaluation framework, nuances in the coherence or coverage of the\n summaries are not captured with it. It is non-trivial\n to evaluate such qualities especially for long doc\u0002ument summarization; future work can design expert human evaluations to explore these nuances.\n \n ",
        "thebibliography": " \n Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\n learning to align and translate. arXiv preprint\n arXiv:1409.0473 .\n Sumit Chopra, Michael Auli, Alexander M Rush, and\n SEAS Harvard. 2016. Abstractive sentence summarization with attentive recurrent neural networks. In\n HLT-NAACL. pages 93\u00e2\u0080\u009398.\n Arman Cohan and Nazli Goharian. 2015. Scientific article summarization using citation-context\n and article\u00e2\u0080\u0099s discourse structure. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 390\u00e2\u0080\u0093400. http://aclweb.org/\n anthology/D15-1045.\n Arman Cohan and Nazli Goharian. 2017a. Contextu\u0002alizing citations for scientific summarization using\n word embeddings and domain knowledge. arXiv\n preprint arXiv:1705.08063 .\n \n ",
        "acknowledgement": "\n We thank the three anonymous reviewers for\n their comments and suggestions.\n \n "
    }
]